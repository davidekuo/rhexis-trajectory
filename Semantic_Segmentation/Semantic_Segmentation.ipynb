{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Semantic_Segmentation.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","background_execution":"on","authorship_tag":"ABX9TyN1MqvOzQ9ZkXKvNgt2tJfZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Semantic Segmentation\n","\n","This script utilizes the [MICCAI2021 Cataract Semantic Segmentation Model](https://github.com/RViMLab/MICCAI2021_Cataract_semantic_segmentation) from the [Robotics and Vision in Medicine Lab at King's College of London](https://rvim.online/) to generate pixel level segmentations of our data. For our purposes, the output segmentations will allow us to identify anatomical landmarks in our images and analyze their positions relative to our surgical trajectories, which can be important aspects of feedback for surgical trainees.\n","> [**Effective Semantic Segmentation in Cataract Surgery: What matters most?**](https://arxiv.org/pdf/2108.06119),            \n","> [Theodoros Pissas*](https://rvim.online/author/theodoros-pissas/), [Claudio S. Ravasio*](https://rvim.online/author/claudio-ravasio/), [Lyndon Da Cruz](), [Christos Bergeles](https://rvim.online/author/christos-bergeles/)  (* equal contribution) <br>\n",">\n","> *arXiv technical report ([arXiv 2108.06119](https://arxiv.org/pdf/2108.06119))*\n",">\n","> *MICCAI 2021 ([proceedings](https://link.springer.com/chapter/10.1007/978-3-030-87202-1_49))*\n","\n"],"metadata":{"id":"Vsnh3tsWC2L1"}},{"cell_type":"markdown","source":["# ‚öôÔ∏è  Set Up\n","\n","#### ‚ö†Ô∏è **This script must be run on a CUDA GPU**\n","To run with a GPU, go to the COLAB taskbar above and go to `Runtime > Change runtime type` and select `GPU` under hardware accelerator. *COLAB will remember this preference the next time you run this ipynb file.*"],"metadata":{"id":"YLbY0u-eES3y"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"9JRKZ6CUCzCO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654019031256,"user_tz":420,"elapsed":4716,"user":{"displayName":"Ben Viggiano","userId":"09742583875951657252"}},"outputId":"be684acb-ffbc-4560-fd05-3306874fb66b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ttach in /usr/local/lib/python3.7/dist-packages (0.0.3)\n","The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["# To begin, run this cell (NO NEED TO EDIT)\n","\n","# Mount our Google Drive to access files\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Import and install all needed libaries\n","import torch\n","import tensorboard\n","import h5py\n","import matplotlib\n","import numpy as np\n","import scipy\n","import cv2\n","import pandas\n","import PIL\n","import future\n","!pip install ttach\n","import ttach\n","import tqdm\n","import os\n","import sys\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","source":["You will now need to specify where your images are located. Please specify the absolute path to the directory containing the `images` folder of our dataset in your Google Drive. (If you are utilizing our dataset downloaded from our [Google Drive link](https://drive.google.com/drive/folders/1QUk7AXNivhF9SRqwJA2lCihnp-nO8Juh?usp=sharing), you should specify the location of this `datasets` folder in your Google Drive.)\n","\n","Additionally, we will also need to specify the REPO_LOC to access our code."],"metadata":{"id":"UXLmYEOiEyvh"}},{"cell_type":"code","source":["# Specify the location of the dataset folder (EDIT REQUIRED)\n","DATA_LOC = \"/content/drive/MyDrive/Rhexis/datasets/manual_trajectories\"\n","\n","# Specify the location of the repo folder (EDIT REQUIRED)\n","REPO_LOC = \"/content/drive/MyDrive/Stanford/rhexis-trajectory\""],"metadata":{"id":"6rZwKbz4ESHr","executionInfo":{"status":"ok","timestamp":1654019031549,"user_tz":420,"elapsed":296,"user":{"displayName":"Ben Viggiano","userId":"09742583875951657252"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Task Selection Overview\n","The CADIS dataset is designed to accomdate three different class groupings, organized to focus on different classification 'tasks'. [[1]](https://www.sciencedirect.com/science/article/pii/S1361841521000992#tbl0005)\n","\n","* Task 1\n","  * Focused on differentiating between anatomy and instruments within every frame.\n","\n","\n","* Task 2\n","  * Includes seperate labels for various instrument types to allow specific instrument classification.\n","\n","\n","* Task 3\n","  * Includes seperate labels for various instrument types to allow specific instrument classification, and also differentiates classes between instrutment handles and tips.\n","\n","\n","Specify which task you would like to generate labels with below:</br>\n","`task = 1` will create labels using task one </br>\n","`task = 2` will create labels using task two </br>\n","`task = 3` will create labels using task three </br>"],"metadata":{"id":"cZpLclBq88ch"}},{"cell_type":"code","source":["# Specify which task you would like to utilize (EDIT REQUIRED)\n","task = 2"],"metadata":{"id":"g-7CbjTS_HOC","executionInfo":{"status":"ok","timestamp":1654019031549,"user_tz":420,"elapsed":2,"user":{"displayName":"Ben Viggiano","userId":"09742583875951657252"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## üëÅ  Generate Labels\n","\n","Run the following cells to generate the requested labels. The labels will be saved to a new directory inside of the `DATA_LOC` location specified above."],"metadata":{"id":"GZGgOdecHa7E"}},{"cell_type":"code","source":["# Change directory to Semantic Segmentation directory to access files\n","cwd = os.path.join(REPO_LOC,\"Semantic_Segmentation\")\n","%cd $cwd\n","\n","# Import custom utils python module\n","from utils import * \n","\n","# Load model object\n","model = configure_segmentation_model(task)"],"metadata":{"id":"Kwx9o_PGHswX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654019032641,"user_tz":420,"elapsed":1094,"user":{"displayName":"Ben Viggiano","userId":"09742583875951657252"}},"outputId":"223e6ad8-9c19-4dce-dcc9-8aa3f83576c6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Stanford/rhexis-trajectory/Semantic_Segmentation\n","Loading model from saved checkpoint...\n","Model successfully loaded from:\n","/content/drive/MyDrive/Stanford/rhexis-trajectory/Semantic_Segmentation/segmentation_models/model_task2/chkpts/chkpt_best.pt\n"]}]},{"cell_type":"code","source":["# Load in images\n","#subdir_names = ['train_set', 'val_set', 'test_set']\n","# expert, pgy2, pgy4\n","subdir_names = ['pgy2']\n","x, img_data = read_in_images(DATA_LOC, subdir_names)"],"metadata":{"id":"AGWflyVroqm0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7848cc86-da8c-4039-c0ab-67d23a68e464"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading in images:\n","3747 image files detected\n"]}]},{"cell_type":"code","source":["# Forward pass our data through the model to get labels\n","create_labels(model, x, img_data, DATA_LOC, subdir_names, test_mode = False)"],"metadata":{"id":"S_7xYMnj5f_e"},"execution_count":null,"outputs":[]}]}
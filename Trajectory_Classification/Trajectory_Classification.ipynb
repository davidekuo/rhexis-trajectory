{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Trajectory_Classification.ipynb","provenance":[],"collapsed_sections":["Wqy6CwcXMiTE"],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MsL1KUUCSzlf"},"outputs":[],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Import libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os"],"metadata":{"id":"b7isWtABS9py"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load data"],"metadata":{"id":"Xpi5di5SMggx"}},{"cell_type":"markdown","source":["## Manually Labeled Trajectories"],"metadata":{"id":"Wqy6CwcXMiTE"}},{"cell_type":"code","source":["# Load data into pandas\n","expert = pd.read_csv(\"/content/drive/MyDrive/datasets/manual_trajectories/expert.csv\")\n","pgy4 = pd.read_csv(\"/content/drive/MyDrive/datasets/manual_trajectories/pgy4.csv\")\n","pgy2 = pd.read_csv(\"/content/drive/MyDrive/datasets/manual_trajectories/pgy2.csv\")\n","trajectories = [expert, pgy4, pgy2]"],"metadata":{"id":"O7NKRNCxTJbU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Keypoint Generated Paths"],"metadata":{"id":"3-pPVvnDMmYh"}},{"cell_type":"code","source":["def load_path(filename):\n","  '''Reads the coordinates from a path file into a dataframe.\n","\n","  Frames where a forceps was not detected are filtered out.\n","\n","  Args:\n","    filename: The path filepath to load from\n","  \n","  Returns:\n","    A pandas dataframe of the coordinates where the forceps is present, sorted\n","      by frame\n","  '''\n","  data = pd.read_csv(filename)\n","  return data[data.key_L_x.notnull()].sort_values(by=['frame_num'])"],"metadata":{"id":"lWIBFEbIMtrI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def file_label(filename):\n","  '''Determines the label for data from a file based on the filename.\n","\n","  Args:\n","    filename: The path filepath to determine based on\n","\n","  Returns:\n","    A number representing the true class of the data:\n","      0 = Junior Resident\n","      1 = Senior Resident\n","      2 = Expert\n","  '''\n","  if filename.startswith(\"Medi\"):\n","    return 0\n","  elif filename.startswith((\"KY\", \"AC\")):\n","    return 1\n","  elif filename.startswith((\"Cataract\", \"SQ\")):\n","    return 2\n","  else:\n","    raise Exception(\"Unhandled filetype: \" + filename)"],"metadata":{"id":"8qojhEv9zMEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data = load_path(\"/content/drive/MyDrive/Projects/rhexis-trajectory/Trajectory_Classification/out.csv\")\n","# data"],"metadata":{"id":"EBz94-655PD0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_path_to_img_matrix(path_df, fill_val_w_frame = False):\n","  '''Converts a list of path coordinates to an image matrix.\n","  In order to handle complexity with floats, the size of the matrix is 10x the\n","  number of pixels, and each point is rounded to the nearest one-tenth pixel.\n","\n","  The matrix is also bounded such that the bottom-left pixel in the trajectory\n","  is placed at (0, 0), by shifting all points by the minimum coordinate.\n","\n","  Additionally, there are two modes in which the matrix can be filled:\n","    1. Boolean path matrix: Places a \"1\" in each position the forceps was\n","         at any point in the trajectory\n","    2. Frame number matrix: Places the frame number in which the forceps was\n","         at a location. This may help with incorporate temporal quality.\n","\n","  Args:\n","    path_df: the path coordinate dataframe to convert\n","    fill_val_w_frame: Whether to fill in the frame number as the coordinate\n","      value. Defaults to False.\n","\n","  Returns;\n","    The 2D np array representing the trajectory\n","  '''\n","  lf_coords = path_df[[\"key_L_x\", \"key_L_y\", \"frame_num\"]]\n","  lf_coords[\"key_L_x\"] = lf_coords[\"key_L_x\"].subtract(lf_coords[\"key_L_x\"].min()) \n","  lf_coords[\"key_L_y\"] = lf_coords[\"key_L_y\"].subtract(lf_coords[\"key_L_y\"].min())\n","  lf_coords = np.rint(lf_coords * 10).astype(int)\n","\n","  maxes = lf_coords.max()\n","  arr = np.zeros((maxes[0], maxes[1]))\n","  lf_np = lf_coords.to_numpy()\n","  arr[lf_np[:, 0]-1, lf_np[:, 1]-1] = lf_np[:, 2] if fill_val_w_frame else 1\n","  return arr"],"metadata":{"id":"sGIyeHiyUUyo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_path_to_std_pupil_matrix(path_df, size=None, fill_val_w_frame=False, verbose=False):\n","  '''Maps a path to a normalized matrix representing position relative to the\n","  center of a pupil.\n","\n","  Args:\n","    path_df: The path coordinate dataframe to convert\n","    size: If set, maps the pupil to a specified size matrix. If unset, uses\n","      the pupil height and width for the matrix.\n","    fill_val_w_frame: Whether to fill in the frame number as the coordinate\n","      value. Defaults to False.\n","  '''\n","  # Determine the pupil size initial matrix\n","  pupil_width = abs(path_df[\"pupil_right_x\"] - path_df[\"pupil_left_x\"]).max()\n","  pupil_height = abs(path_df[\"pupil_up_y\"] - path_df[\"pupil_down_y\"]).max()\n","  arr = np.zeros(size or (pupil_width, pupil_height))\n","  arr_center_x, arr_center_y = (np.array(arr.shape) / 2).astype(int)\n","  # return center_x, center_y\n","\n","  for i in range(len(path_df)):\n","    # Calculate how much to translate forceps point to center pupil at center\n","    # of matrix\n","    # pupil_center_x = path_df[\"pupil_center_x\"].iloc[i]\n","    # pupil_center_y = path_df[\"pupil_center_y\"].iloc[i]\n","    # translation_factor_x = arr_center_x - pupil_center_x \n","    # translation_factor_y = arr_center_y - pupil_center_y\n","    translation_factor_x = path_df[\"pupil_left_x\"].iloc[i]\n","    translation_factor_y = path_df[\"pupil_down_y\"].iloc[i]\n","    if verbose:\n","      print(\"=======================================================\")\n","      print(\"Translate: \" + str((translation_factor_x, translation_factor_y)))\n","\n","    # Calculate scale factor for forceps\n","    # Assume elliptical if tilted. Find max between both sides to ensure one side\n","    # is not clipped\n","    radius_x = max(path_df[\"pupil_center_x\"].iloc[i] - path_df[\"pupil_left_x\"].iloc[i],\n","                   path_df[\"pupil_right_x\"].iloc[i] - path_df[\"pupil_center_x\"].iloc[i])\n","    radius_y = max(path_df[\"pupil_center_y\"].iloc[i] - path_df[\"pupil_up_y\"].iloc[i],\n","                   path_df[\"pupil_down_y\"].iloc[i] - path_df[\"pupil_center_y\"].iloc[i])\n","    scale_factor_x = arr_center_x / radius_x\n","    scale_factor_y = arr_center_y / radius_y\n","    if verbose:\n","      print(\"Scale: \" + str((scale_factor_x, scale_factor_y)))\n","\n","    coord_l_x = int((path_df[\"key_L_x\"].iloc[i] - translation_factor_x) * scale_factor_x)\n","    coord_l_y = int((path_df[\"key_L_y\"].iloc[i] - translation_factor_y) * scale_factor_y)\n","\n","    if verbose:\n","      print(\"Coord: \" + str((coord_l_x, coord_l_y)))\n","      print(\"=======================================================\")\n","\n","    arr[coord_l_x, coord_l_y] = path_df[\"frame_num\"].iloc[i] if fill_val_w_frame else 1\n","  return arr\n","\n"],"metadata":{"id":"hp5NwCN7j2P5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_pupil_std_data(matrix_size=None, fill_val_w_frame=False):\n","  '''Loads data and performs conversion of paths to standardized pupil centered\n","  matrices.\n","\n","  Args:\n","    matrix_size: The matrix size to project onto for the conversion function\n","    fill_val_w_frame: Whether to use the frame number as the value in the matrix.\n","      Defaults to 1.\n","  \n","  Returns:\n","    Tuple of matrix list and corresponding labels\n","  '''\n","  matrix_size = matrix_size or (100, 100)\n","  DATA_LOC = \"/content/drive/MyDrive/Rhexis/datasets/test_pulls/OUTPUT/\"\n","  files = os.listdir(DATA_LOC)\n","  path_dfs = [load_path(f'{DATA_LOC}/{file}') for file in files]\n","  labels = [file_label(file) for file in files]\n","  path_matrices = [convert_path_to_std_pupil_matrix(df, (100, 100), True) for df in path_dfs]\n","  return path_matrices, labels"],"metadata":{"id":"R2t3_Qi22AAQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad_matrices(path_matrices):\n","  '''Pads the right and up sides of a matrix with 0s to make all matrices in a\n","  list the same shape. Preserves the property that the bottom-left most point\n","  in the trajectory will be at (0,0)\n","\n","  Args:\n","    path_matrices: List of matrices to pad\n","  \n","  Returns:\n","    A list of matrices of the same size\n","  '''\n","  max_x = max(map(lambda x: x.shape[0], path_matrices))\n","  max_y = max(map(lambda x: x.shape[1], path_matrices))\n","  for i, m in enumerate(path_matrices):\n","    path_matrices[i] = np.pad(m, ((0, max_x-m.shape[0]), (0, max_y-m.shape[1])), mode='constant')\n","    print(path_matrices[i].shape)\n","  return path_matrices"],"metadata":{"id":"98G81iduGAev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plt.scatter(nonnull_data.pupil_center_x, nonnull_data.pupil_center_y, c=\"red\")\n","# plt.scatter(nonnull_data.pupil_right_x, nonnull_data.pupil_center_y, c=\"blue\")\n","# plt.scatter(nonnull_data.pupil_left_x, nonnull_data.pupil_center_y, c=\"purple\")\n","# plt.scatter(nonnull_data.pupil_center_x, nonnull_data.pupil_up_y, c=\"teal\")\n","# plt.scatter(nonnull_data.pupil_center_x, nonnull_data.pupil_down_y, c=\"pink\")\n","# plt.scatter(nonnull_data.key_R_x, nonnull_data.key_R_y, c=\"green\")\n","# plt.scatter(nonnull_data.key_L_x, nonnull_data.key_L_y, c=\"yellow\")"],"metadata":{"id":"oJ4Shl879__D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exploratory Visualization"],"metadata":{"id":"TDCZCXG8GBpP"}},{"cell_type":"code","source":["path_matrices = get_pupil_std_data()\n","for m in path_matrices:\n","  plt.imshow(m)\n","  plt.show()"],"metadata":{"id":"RTEr6JXgnN-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## (OLD) Full Trajectories"],"metadata":{"id":"mcaOyU_vGGsW"}},{"cell_type":"code","source":["# Plot PGY2 trajectory\n","for i in range(max(pgy2['pull']) + 1):\n","  plt.scatter(pgy2[pgy2['pull'] == i]['x'], pgy2[pgy2['pull'] == i]['y'])"],"metadata":{"id":"PMa3a9SoTVrK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot PGY4 trajectory\n","for i in range(max(pgy4['pull']) + 1):\n","  plt.scatter(pgy4[pgy4['pull'] == i]['x'], pgy4[pgy4['pull'] == i]['y'])"],"metadata":{"id":"zU-9VSbHTWRZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot expert trajectory\n","for i in range(max(expert['pull']) + 1):\n","  plt.scatter(expert[expert['pull'] == i]['x'], expert[expert['pull'] == i]['y'])"],"metadata":{"id":"zySn1gzGTwrX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## (OLD) Individual Pulls"],"metadata":{"id":"LqeaSOkvGKAk"}},{"cell_type":"code","source":["# Plot all 4 pulls in expert trajectory\n","for i in range(max(expert['pull']) + 1):\n","  plt.figure()\n","  plt.scatter(expert[expert['pull'] == i]['x'], expert[expert['pull'] == i]['y'])\n","plt.show()"],"metadata":{"id":"SrTnXbrNT3XS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot all PGY4 trajectories\n","for i in range(max(pgy4['pull']) + 1):\n","  plt.figure()\n","  plt.scatter(pgy4[pgy4['pull'] == i]['x'], pgy4[pgy4['pull'] == i]['y'])\n","plt.show()"],"metadata":{"id":"T8nC1HKYT3zb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot all PGY2 trajectories\n","for i in range(max(pgy2['pull']) + 1):\n","  plt.figure()\n","  plt.scatter(pgy2[pgy2['pull'] == i]['x'], pgy2[pgy2['pull'] == i]['y'])\n","plt.show()"],"metadata":{"id":"SxTpTrQZT4I8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"xeMzKdoaeezQ"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA"],"metadata":{"id":"-HNSuqpSfIh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pca(n_components):\n","  pca = PCA(n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(X_train)\n"],"metadata":{"id":"7Rulh_PteiUZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Extraction Functions"],"metadata":{"id":"kgAkfAtWGORt"}},{"cell_type":"code","source":["def pull_to_length(pull):\n","    \"\"\"\n","    Args:\n","      pull: The dataframe for the pull to conver to velocities\n","  \n","    Return:\n","      velocities: velocities calculated as distance traveled per frame\n","    \"\"\"\n","    return len(pull.x)"],"metadata":{"id":"KkoGFwBCJaEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pull_to_velocities_and_accelerations(pull):\n","  \"\"\"\n","  Args:\n","    pull: The dataframe for the pull to conver to velocities\n","  \n","  Return:\n","    mean velocity: velocity calculated as distance traveled per frame\n","    mean acceleration: acceleration calculated as change in velocity between frames\n","  \"\"\"\n","  x, y = pull.x, pull.y\n","  velocities = []\n","  accelerations = []\n","\n","  for i in range(len(x)-1):\n","    x1, x2 = x[i:i+2]\n","    y1, y2 = y[i:i+2]\n","    velocities.append(np.sqrt(np.square(x2 - x1) + np.square(y2 - y1)))\n","\n","  for i in range(len(velocities) - 1):\n","    accelerations.append(np.abs(velocities[i+1] - velocities[i]))\n","\n","  return (np.mean(velocities), np.mean(accelerations))"],"metadata":{"id":"y5d3cT1LJd83"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pull_to_angles(pull):\n","  \"\"\"\n","  Args:\n","    pull: The dataframe for the pull to convert to angles\n","\n","  Return:\n","    angles: The angles for each triple of datapoints\n","  \"\"\"\n","  x, y = pull.x, pull.y\n","  angles = []\n","  for i in range(len(x)-2):\n","    x1, x2, x3 = x[i:i+3]\n","    y1, y2, y3 = y[i:i+3]\n","    v1 = np.array([x1-x2, y1-y2])\n","    v2 = np.array([x3-x2, y3-y2])\n","    angle_nocos = np.dot(v1, v2) / (np.linalg.norm(v1, 2) * np.linalg.norm(v2, 2))\n","    angle_floor = np.where(angle_nocos < -1, -1.0, angle_nocos)\n","    angle_ceil = np.where(angle_floor > 1, 1.0, angle_floor) \n","    angle = np.arccos(angle_ceil) * 180 / np.pi\n","    angles.append(angle)\n","  return angles"],"metadata":{"id":"cI3J3TacT5tD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def angles_to_bins(angles, num_bins=20):\n","  \"\"\"\n","  Args:\n","    angles: the list of angles to bin\n","\n","  Returns:\n","    histogram: histogram values\n","    bins: histogram bins\n","  \"\"\"\n","  bins = [i for i in range(0, 181, 180//num_bins)] \n","  return np.histogram(angles, bins)\n"],"metadata":{"id":"j5CPHFv6o0Pp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def featurize_pull(pull):\n","  \"\"\"\n","  Performs all featurizations for an individual pull\n","\n","  Args:\n","    pull: The pull dataframe to convert to a feature row\n","\n","  Returns:\n","    row: A feature row for the pull\n","  \"\"\"\n","  angles = pull_to_angles(pull)\n","  hist, bins = angles_to_bins(angles)\n","\n","  length = pull_to_length(pull)\n","  mean_velocity, mean_accel = pull_to_velocities_and_accelerations(pull)\n","\n","  features = np.append(hist, np.array([length, mean_velocity, mean_accel]))\n","\n","  return features"],"metadata":{"id":"qsz7XL2DwfnH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def featurize_trajectory(trajectory, label, transform_func=None):\n","  \"\"\"\n","  Splits a trajectory by pulls and featurizes each pull. Each pull is assigned\n","  the input label.\n","\n","  Args:\n","    trajectory: The trajectory dataframe to split into featurized pulls\n","    label: The label to assign each pull for the trajectory\n","    transform_func: Custom transformation to use during featurization. Defaults\n","        to `featurize_pull`.\n","  \n","  Return:\n","    X: The feature rows\n","    y: The labels for each row\n","  \"\"\"\n","  transform_func = transform_func or featurize_pull\n","  print(transform_func)\n","  X = [transform_func(df) for val, df in trajectory.groupby('pull')]\n","  y = np.repeat(label, len(X))\n","  return np.array(X), y"],"metadata":{"id":"PvvfWg1MxuKs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def featurize_trajectories(trajectories, transform_func=None):\n","  \"\"\"\n","  Converts a list of trajectories into a feature matrix and label vector\n","\n","  Args:\n","    trajectory: The trajctories list to split into featurized pulls\n","    transform_func: Custom transformation to use during featurization. Defaults\n","        to `featurize_pull`.\n","  \n","  Return:\n","    X: The feature matrix\n","    y: The label vector\n","  \"\"\"\n","  featurized_trajectory_list = [featurize_trajectory(t, i, transform_func) for i, t in enumerate(trajectories)]\n","  X = np.row_stack([x for x, y in featurized_trajectory_list])\n","  y = np.concatenate([y for x, y in featurized_trajectory_list])\n","  return X, y"],"metadata":{"id":"4BcrYbiWGZbQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_pulls(trajectories):\n","  \"\"\"\n","  Returns raw pulls by modifying featurization method to apply identity function\n","\n","  Args:\n","    trajectories: The trajctories dataframe to extract pulls from\n","  \"\"\"\n","  return featurize_trajectories(trajectories, lambda x: x)"],"metadata":{"id":"55W2R4zhweqU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Fitting/Prediction"],"metadata":{"id":"ZMDkYgLpGWMZ"}},{"cell_type":"code","source":["X, y = get_pupil_std_data(trajectories)\n","y"],"metadata":{"id":"ov9GNoMDH-jq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X"],"metadata":{"id":"kL7wMJ8orMkz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Keep deterministic\n","np.random.seed(42)\n","sss = StratifiedShuffleSplit(1, test_size=.2)\n","train_ind, test_ind =  next(sss.split(X, y))\n","X_train, X_test = X[train_ind], X[test_ind]\n","y_train, y_test = y[train_ind], y[test_ind]\n","# Print to check class balance\n","y_train, y_test"],"metadata":{"id":"QCmo_9oe1Exx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Logistic Model (Baseline)"],"metadata":{"id":"TkaP8GMeW1z7"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression(random_state=0, multi_class=\"multinomial\", max_iter=1000).fit(X_train, y_train)\n","clf.score(X_test, y_test)"],"metadata":{"id":"EWe-kjPPArPa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Quadratic GDA"],"metadata":{"id":"JzQqqhcqW8ju"}},{"cell_type":"markdown","source":["This does not perform well"],"metadata":{"id":"FOI-KXKXkYrN"}},{"cell_type":"code","source":["from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","\n","clf = QuadraticDiscriminantAnalysis()\n","clf.fit(X_train, y_train)\n","clf.score(X_test, y_test)"],"metadata":{"id":"v5sQCJ7DilBN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Unsupervised clustering"],"metadata":{"id":"RF-2jz4vkcaW"}},{"cell_type":"code","source":["from sklearn.cluster import MiniBatchKMeans\n","from sklearn.metrics import accuracy_score\n","\n","BATCH_SIZE = 3\n","kmeans = MiniBatchKMeans(n_clusters=3,\n","                          random_state=0,\n","                          batch_size=BATCH_SIZE)\n","for i in range(0, len(X_train), BATCH_SIZE):\n","  kmeans.partial_fit(X_train[i:i+BATCH_SIZE])\n","\n","y_pred = kmeans.predict(X_test)\n","accuracy_score(y_test, y_pred)"],"metadata":{"id":"SNYmvMvTkftr"},"execution_count":null,"outputs":[]}]}